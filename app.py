# -*- coding: utf-8 -*-
"""app

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1j_g4sod2CU8L0r57J6STsm6EYxu32eZ8
"""

import streamlit as st
import pandas as pd
import joblib
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, matthews_corrcoef, confusion_matrix, classification_report

st.title("Credit Card Default Prediction App")

uploaded_file = st.file_uploader("Upload Test CSV File", type=["csv"])

model_choice = st.selectbox(
    "Select Model",
    ["Logistic Regression", "Decision Tree", "KNN", "Naive Bayes", "Random Forest", "XGBoost"]
)

model_paths = {
    "Logistic Regression": "saved_models/logistic.pkl",
    "Decision Tree": "saved_models/tree.pkl",
    "KNN": "saved_models/knn.pkl",
    "Naive Bayes": "saved_models/bayes.pkl",
    "Random Forest": "saved_models/forest.pkl",
    "XGBoost": "saved_models/xgboost.pkl"
}

if uploaded_file is not None:

    df = pd.read_csv(uploaded_file)

    # Remove unwanted index columns
    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]

    # Drop ID column if exists
    if "ID" in df.columns:
        df = df.drop(columns=["ID"])

    # Handle different target column names
    if "default payment next month" in df.columns:
        df = df.rename(columns={"default payment next month": "credit_default"})

    if "Y" in df.columns:
        df = df.rename(columns={"Y": "credit_default"})

    if "credit_default" not in df.columns:
        st.error("CSV must contain target column: credit_default")
    else:

        X = df.drop("credit_default", axis=1)
        y_true = df["credit_default"]

        model = joblib.load(model_paths[model_choice])
        scaler = joblib.load("saved_models/scaler.pkl")

        # IMPORTANT FIX: use .values to avoid feature name mismatch
        if model_choice in ["Logistic Regression", "KNN"]:
            X = scaler.transform(X.values)

        y_pred = model.predict(X)

        if hasattr(model, "predict_proba"):
            y_prob = model.predict_proba(X)[:, 1]
            auc = roc_auc_score(y_true, y_prob)
        else:
            auc = "N/A"

        accuracy = accuracy_score(y_true, y_pred)
        precision = precision_score(y_true, y_pred)
        recall = recall_score(y_true, y_pred)
        f1 = f1_score(y_true, y_pred)
        mcc = matthews_corrcoef(y_true, y_pred)

        st.subheader("Evaluation Metrics")
        st.write(f"Accuracy: {accuracy:.4f}")
        st.write(f"AUC: {auc}")
        st.write(f"Precision: {precision:.4f}")
        st.write(f"Recall: {recall:.4f}")
        st.write(f"F1 Score: {f1:.4f}")
        st.write(f"MCC: {mcc:.4f}")

        st.subheader("Confusion Matrix")
        cm = confusion_matrix(y_true, y_pred)
        st.write(cm)

        st.subheader("Classification Report")
        st.text(classification_report(y_true, y_pred))
